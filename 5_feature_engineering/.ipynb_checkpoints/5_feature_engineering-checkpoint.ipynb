{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# フィーチャーエンジニアリング\n",
    "\n",
    "このノートブックでは、TensorFlow でフィーチャーエンジニアリングを行う方法を学習します。\n",
    "\n",
    "## 目的\n",
    "- TensorFlow でフィーチャーエンジニアリングを行う方法を学習する\n",
    "- End to End のモデルの作成〜デプロイを学ぶ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, subprocess\n",
    "import numpy as np\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version: \",tf.version.VERSION)\n",
    "\n",
    "cmd = 'gcloud config list project --format \"value(core.project)\"'\n",
    "PROJECT = subprocess.Popen(\n",
    "      cmd, stdout=subprocess.PIPE,\n",
    "      shell=True, universal_newlines=True).stdout.readlines()[0].rstrip('\\n')\n",
    "print(\"Your current GCP Project Name is: {}\".format(PROJECT))\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "BUCKET = \"{}-keras\".format(PROJECT)\n",
    "\n",
    "# Do not change these\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BUCKET\"] = BUCKET # DEFAULT BUCKET WILL BE <PROJECT ID>-keras\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # SET TF ERROR LOG VERBOSITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python の trainer パッケージを作成する\n",
    "\n",
    "今回も、AI Platformのトレーニングに送信する `trainer` パッケージを作成します。\n",
    "\n",
    "フィーチャーエンジニアリングに関する前回との差分に注目してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Remove files from previous notebook runs.\n",
    "rm -rf taxifare_tf2\n",
    "\n",
    "mkdir taxifare_tf2\n",
    "touch taxifare_tf2/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初に、必要なパッケージのインストールを行いましょう。（前ラボと同内容）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile taxifare_tf2/model.py\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column as fc\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "# set TF error log verbosity\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "すべてのデータ列名、予測するデータ列（ラベル）名、そしてデフォルトの値を定義します。（前ラボと同内容）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a taxifare_tf2/model.py \n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    'fare_amount',\n",
    "    'hourofday',\n",
    "    'dayofweek',\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude'\n",
    "]\n",
    "\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "\n",
    "CATEGORICAL_COLS = ['hourofday', 'dayofweek']\n",
    "NUMERIC_COLS = ['pickup_longitude', 'pickup_latitude',\n",
    "                'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "# Needed to impute for missing values.\n",
    "\n",
    "DEFAULTS = [[0.0], [0], [0], [0.0], [0.0], [0.0], [0.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、使用する特徴量と予測するラベルを定義し、トレーニング用データセットを読み込みます。（前回と同内容）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a taxifare_tf2/model.py \n",
    "\n",
    "# Keras models expect a dictionary of features and a label as training inputs.\n",
    "def features_and_labels(row_data):\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label\n",
    "\n",
    "\n",
    "def load_dataset(pattern, batch_size=1, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(pattern,\n",
    "                                                    batch_size,\n",
    "                                                    CSV_COLUMNS,\n",
    "                                                    DEFAULTS)\n",
    "    dataset = dataset.map(features_and_labels)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "        # take advantage of multi-threading!! 1=AUTOTUNE\n",
    "        dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## フィーチャーエンジニアリング\n",
    "さて、ここからフィーチャーエンジニアリングに関する処理を追加していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a taxifare_tf2/model.py \n",
    "\n",
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff*londiff + latdiff*latdiff)\n",
    "\n",
    "#Rescale latitude and longitude to be values in the interval [0,1]\n",
    "\n",
    "def scale_longitude(lon_column):\n",
    "    return (lon_column + 78)/8.0\n",
    "\n",
    "def scale_latitude(lat_column):\n",
    "    return (lat_column - 37)/8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、 `transform` 関数を追加しましょう。この関数は二つの役割を持っています。<br>\n",
    "まず、上のセルで定義した変換処理を、 `Lambda` レイヤーとして Keras に追加します。これにより、トレーニング時と推論時のデータ前処理の差分などを気にすること無く、モデルの内部に変換処理を実装することができます。<br>\n",
    "そして、この関数を 後ほど作成する DNN への入力として利用する`feature_columns` の定義に使います。このセルには多くの内容が含まれていますので、気をつけて読み進めてください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a taxifare_tf2/model.py \n",
    "\n",
    "def transform(inputs, numeric_cols, string_cols, nbuckets):\n",
    "    print(\"Inputs before features transformation: {}\".format(inputs.keys()))\n",
    "\n",
    "    # Pass-through columns\n",
    "    transformed = inputs.copy()\n",
    "\n",
    "    feature_columns = {\n",
    "        colname: tf.feature_column.numeric_column(colname)\n",
    "        for colname in numeric_cols\n",
    "    }\n",
    "\n",
    "    # Scaling longitude from range [-70, -78] to [0, 1]\n",
    "    for lon_col in ['pickup_longitude', 'dropoff_longitude']:\n",
    "        transformed[lon_col] = layers.Lambda(\n",
    "            scale_longitude,\n",
    "            name=\"scale_{}\".format(lon_col))(inputs[lon_col])\n",
    "\n",
    "    # Scaling latitude from range [37, 45] to [0, 1]\n",
    "    for lat_col in ['pickup_latitude', 'dropoff_latitude']:\n",
    "        transformed[lat_col] = layers.Lambda(\n",
    "            scale_latitude,\n",
    "            name='scale_{}'.format(lat_col))(inputs[lat_col])\n",
    "\n",
    "    # Add Euclidean distance\n",
    "    transformed['euclidean'] = layers.Lambda(\n",
    "        euclidean,\n",
    "        name='euclidean')([inputs['pickup_longitude'],\n",
    "                           inputs['pickup_latitude'],\n",
    "                           inputs['dropoff_longitude'],\n",
    "                           inputs['dropoff_latitude']])\n",
    "    feature_columns['euclidean'] = fc.numeric_column('euclidean')\n",
    "    \n",
    "    day_fc = fc.categorical_column_with_identity('dayofweek', 7)\n",
    "    hour_fc = fc.categorical_column_with_identity('hourofday', 24)\n",
    "\n",
    "    # Create bucketized features\n",
    "    latbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    b_plat = fc.bucketized_column(\n",
    "        feature_columns['pickup_latitude'], latbuckets)\n",
    "    b_dlat = fc.bucketized_column(\n",
    "        feature_columns['dropoff_latitude'], latbuckets)\n",
    "    b_plon = fc.bucketized_column(\n",
    "        feature_columns['pickup_longitude'], lonbuckets)\n",
    "    b_dlon = fc.bucketized_column(\n",
    "        feature_columns['dropoff_longitude'], lonbuckets)\n",
    "\n",
    "    # Create crossed columns\n",
    "    ploc = fc.crossed_column([b_plat, b_plon], nbuckets * nbuckets)\n",
    "    dloc = fc.crossed_column([b_dlat, b_dlon], nbuckets * nbuckets)\n",
    "    pd_pair = fc.crossed_column([ploc, dloc], nbuckets ** 4)\n",
    "    \n",
    "    day_hr_pair = fc.crossed_column([day_fc, hour_fc], 24*7)\n",
    "\n",
    "    # Create embedding columns\n",
    "    feature_columns['pickup_and_dropoff'] = fc.embedding_column(pd_pair, 100)\n",
    "    feature_columns['day_hour'] = fc.embedding_column(day_hr_pair, 4)\n",
    "\n",
    "    print(\"Transformed features: {}\".format(transformed.keys()))\n",
    "    print(\"Feature columns: {}\".format(feature_columns.keys()))\n",
    "    return transformed, feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "最後に、DNNのアーキテクチャを実装します。<br>\n",
    "前ラボのモデルと比較して、どのようにフィーチャーエンジニアリングが適用されているかを見比べてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a taxifare_tf2/model.py \n",
    "\n",
    "# Create custom metric for training\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "# Build DNN Model\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    NBUCKETS = 10\n",
    "    \n",
    "    # Input layer is all floats except for hourofday and dayofweek which are integers.\n",
    "    inputs = {\n",
    "        colname: layers.Input(name=colname, shape=(), dtype='float32')\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "    inputs.update({\n",
    "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
    "        for colname in CATEGORICAL_COLS\n",
    "    })\n",
    "\n",
    "    # Transforms\n",
    "    transformed, feature_columns = transform(inputs,\n",
    "                                             numeric_cols=NUMERIC_COLS,\n",
    "                                             string_cols=CATEGORICAL_COLS,\n",
    "                                             nbuckets=NBUCKETS)\n",
    "    dnn_inputs = layers.DenseFeatures(feature_columns.values())(transformed)\n",
    "\n",
    "    # Two hidden layers of [32, 8] \n",
    "    h1 = layers.Dense(32, activation='relu', name='h1')(dnn_inputs)\n",
    "    h2 = layers.Dense(8, activation='relu', name='h2')(h1)\n",
    "\n",
    "    # final output is a linear activation because this is regression\n",
    "    output = layers.Dense(1, activation='linear', name='fare')(h2)\n",
    "    model = models.Model(inputs, output)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[rmse])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、トレーニングプロセスを管理する関数を作成します。<br>\n",
    "`args` の辞書は、 トレーニングの際に`task.py` を通してコマンドラインから渡されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a taxifare_tf2/model.py \n",
    "\n",
    "def train_and_export_model(args):\n",
    "    TRAIN_BATCH_SIZE = 128 \n",
    "    NUM_TRAIN_EXAMPLES = 50000 * args['train_epochs']\n",
    "    NUM_EVALS = args['train_epochs']\n",
    "    NUM_EVAL_EXAMPLES = 15000\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = build_model()\n",
    "\n",
    "    trainds = load_dataset(args['train_data_path'],\n",
    "                           TRAIN_BATCH_SIZE * 4,\n",
    "                           tf.estimator.ModeKeys.TRAIN)\n",
    "    evalds = load_dataset(args['eval_data_path'],\n",
    "                          1000,\n",
    "                          tf.estimator.ModeKeys.EVAL).take(NUM_EVAL_EXAMPLES//1000)\n",
    "\n",
    "    steps_per_epoch = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=args['output_dir']),\n",
    "                tf.keras.callbacks.TensorBoard(args['output_dir'], histogram_freq=1, write_grads=True)]\n",
    "\n",
    "    history = model.fit(trainds,\n",
    "                        verbose=2,\n",
    "                        validation_data=evalds,\n",
    "                        epochs=NUM_EVALS,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `task.py` ファイルを作成する\n",
    "\n",
    "これで `model.py` は完成です。<br>\n",
    "前ラボと同様に、トレーニングジョブを管理する `task.py` を作成しましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile taxifare_tf2/task.py \n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help = \"GCS or local path to training data\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_epochs\",\n",
    "        help = \"Steps to run the training job for (default: 5)\",\n",
    "        type = int,\n",
    "        default = 5\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help = \"GCS or local path to evaluation data\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help = \"GCS location to write checkpoints and export models\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help = \"This is not used by our model, but it is required by gcloud\",\n",
    "    )\n",
    "    args = parser.parse_args().__dict__\n",
    "\n",
    "    model.train_and_export_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルをローカルでテストする\n",
    "\n",
    "AI Platform でトレーニングを行う前に、パッケージが動作するかどうかをローカルで確認しましょう。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./taxi_model\n",
    "!python3 -m taxifare_tf2.task \\\n",
    "    --train_data_path=../data/taxi-train.csv \\\n",
    "    --eval_data_path=../data/taxi-valid.csv \\\n",
    "    --train_epochs=3 \\\n",
    "    --output_dir=./local_taxi_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud AI Platform でトレーニングを実行する\n",
    "\n",
    "全てがローカルで動作することが確認できましたので、いよいよクラウドで実行しましょう。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp -r ../data/ gs://{BUCKET}/taxifare_fe_example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = \"gs://{}/taxifare_fe_example/saved_model/\".format(BUCKET)\n",
    "TRAIN_DATA_PATH = \"gs://{}/taxifare_fe_example/data/taxi-train.csv\".format(BUCKET)\n",
    "EVAL_DATA_PATH = \"gs://{}/taxifare_fe_example/data/taxi-valid.csv\".format(BUCKET)\n",
    "\n",
    "!gsutil -m rm -rf {OUTDIR} # start fresh each time\n",
    "!gcloud ai-platform jobs submit training taxifare_$(date -u +%y%m%d_%H%M%S) \\\n",
    "    --package-path=taxifare_tf2 \\\n",
    "    --module-name=taxifare_tf2.task \\\n",
    "    --job-dir=gs://{BUCKET}/taxifare_example \\\n",
    "    --python-version=3.7 \\\n",
    "    --runtime-version=2.1 \\\n",
    "    --region={REGION}\\\n",
    "    --scale-tier BASIC \\\n",
    "    -- \\\n",
    "    --train_data_path={TRAIN_DATA_PATH} \\\n",
    "    --eval_data_path={EVAL_DATA_PATH}  \\\n",
    "    --train_epochs=20 \\\n",
    "    --output_dir={OUTDIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard でトレーニングをモニタリングする\n",
    "\n",
    "以下の手順でTensorboard を実行してください。\n",
    "- 以下のセルを実行し、出力を Cloud Shell に貼り付けて実行\n",
    "- Cloud Shell 右上の Web Preview -> Preview on Port 8080 をクリック\n",
    "- Tensorboard を確認する（正しく表示されない場合は、しばらく待ち、ブラウザを更新してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tensorboard --logdir {} --port 8080'.format(OUTDIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニングは 5-7 分ほどで完了します。<br>\n",
    "完了したら、以下のセルを実行し、 `SavedModel` が保存されていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -r {OUTDIR}*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルをデプロイする\n",
    "\n",
    "では、エクスポートされた `SavedModel` をデプロイして、`v2`のバージョン名をつけましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION='v2'\n",
    "\n",
    "!gcloud ai-platform versions create {VERSION} --model taxifare \\\n",
    "    --origin {OUTDIR} \\\n",
    "    --python-version=3.7 \\\n",
    "    --runtime-version 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デフォルトのバージョンを変更します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform versions set-default v2 --model=taxifare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、先ほどと同様に午後6時に[ウォールストリートからブライアント公園まで](https://www.google.com/maps/dir/%E3%82%A6%E3%82%A9%E3%83%BC%E3%83%AB%E8%A1%97,+%E3%82%A2%E3%83%A1%E3%83%AA%E3%82%AB%E5%90%88%E8%A1%86%E5%9B%BD+New+York/Bryant+Park,+%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A8%E3%83%BC%E3%82%AF+%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A8%E3%83%BC%E3%82%AF%E5%B7%9E+%E3%82%A2%E3%83%A1%E3%83%AA%E3%82%AB%E5%90%88%E8%A1%86%E5%9B%BD/@40.7302283,-74.0247121,13z/data=!3m1!4b1!4m13!4m12!1m5!1m1!1s0x89c25a165bedccab:0x2cb2ddf003b5ae01!2m2!1d-74.0088256!2d40.7060361!1m5!1m1!1s0x89c259aae7a0b1bd:0xb49cafb82537f1a7!2m2!1d-73.9832326!2d40.7535965)の4マイルの距離をタクシー移動する際の料金を予測してみましょう。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build(\"ml\", \"v1\", credentials = credentials,\n",
    "            discoveryServiceUrl = \"https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json\")\n",
    "\n",
    "request_data = {\"instances\":\n",
    "  [\n",
    "      {\n",
    "        \"dayofweek\": 1,\n",
    "        \"hourofday\": 18,\n",
    "        \"pickup_longitude\": -73.0101638,\n",
    "        \"pickup_latitude\": 40.7059409,\n",
    "        \"dropoff_longitude\": -73.9834672,\n",
    "        \"dropoff_latitude\": 40.7532916,\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = \"projects/{}/models/taxifare\".format(PROJECT) # use default version\n",
    "\n",
    "response = api.projects().predict(body = request_data, name = parent).execute()\n",
    "print(\"response = {0}\".format(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
